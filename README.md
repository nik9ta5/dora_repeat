# Воспроизведение результатов статьи DoRA: Weight-Decomposed Low-Rank Adaptation 

* [archive](https://arxiv.org/html/2402.09353v6) 
* [ICML 2024](https://openreview.net/forum?id=3d5CIRG1n2)

---
Директория `peft/` скопирована из репозитория [GitHub](https://github.com/NVlabs/DoRA)

Актуальный блокнот: `index_pipeline_ft.ipynb`

PS: раньше был `indexGemma1B.ipynb`

PSS: **под тонкой настройкой подразумевается PEFT**, так как на конфигурации оборудования, на которой производились тесты и соответствующие запуски, провести полную FT не представляется возможным.

---
## Работа

1. В работе исследуется применение методов PEFT, для моделей разных размеров (от 7B до 13B) и тонкой настройки на соответствующих датасетах (приведены в работе).
Сравнение полученных результатов (таблица 1 в работе). В работе утвердается, что при использовании метода DoRA, точность модели достигается как при полной тонкой настройки, но естественно осущестлвяется эффективней, так как обновляются только адаптеры, а не все параметры, как при тонкой настройки, что подтвержается результами сравнения, приведенными в таблице 1 (в работе - подразумевается в статье авторов "DoRA: Weight-Decomposed Low-Rank Adaptation").

2. Было принято решение отойти от работы и определить задачи для рассмотрения. 
Будет произведено сравнение только двух методов PEFT - **LoRA** и **DoRA**.
В качестве модели используется казуальная (как Ламы в работе) [Gemma-3 1B](https://huggingface.co/google/gemma-3-1b-it) (параметров значительно меньше). Выбор модели был произведен чисто из соображений к вычислительным ресурсам (можно выбрать другую казуальную модель).

- В качестве решаемой задачи - ответ на вопрос по контексту.
- В качестве датасета и бенчмарка **SQuAD 2.0** (тренировочная и валидационная части).
- Предобработка датасета для тонкой настройки.
- Метрики **Exact Match** и **F1**.
- Тонкая настройка в течении 1-ой эпохи (для LoRA и для DoRA по эпохе, чисто в рамках демонстрации, конечно, для получения более качественных результатов потребуется больше).

3. FT
* Оценка модели будет проводиться на валидацонной части **SQuAD 2.0** с разными промптами, температурами. Будут преведены соответствующие файлы логов, для аргументации выбора того или иного промпта.
* PEFT с **LoRA**, оценка на валидационной части.
* PEFT c **DoRA**, оценка на валидационной части.
* Сравнение результатов. Выводы 


**Модель загружается с Hugging Face (необходимо получить доступ к репозиторию, модель закрыта)**

**SQuAD 2.0 загружается из HF**

---

## Файлы

1. `index_pipeline_ft.ipynb` - блокнот с пайплайном FT
2. `index_dataset_preprocess.ipynb` - блокнот с предобработкой датасета
3. `eval_models_out/` - директория с файлами логов запуска обучения/оценки модели
4. `lora_ft_squad2/` - директория с адаптерами LoRA, полученными в результате FT
5. `dora_ft_squad2` - директория с адаптерами DoRA, полученными в результате FT

---

## Тонкая настройка

Процесс расписан в `index_pipeline_ft.ipynb` и сделаны соответствующие выводы.

---

## Логи

В конце файлов логов:

`model_promptUpdate_eval_20-05-2025_22-07-40.log` - EM: 0.21612060978691147 F1: 0.3843759597719857

`model_promptUpdate_eval_20-05-2025_23-55-48.log` - EM: 0.15244672786995705 F1: 0.2501595377793837

`model_promptUpdate_eval_21-05-2025_00-34-10.log` - EM: 0.14697212162048345 F1: 0.246137657941505

Приведены метрики EM и F1. Так же приведены все ответы языковой модели, вместе с промптом и правильным ответом. Оценка производилась на валидаионной части датасета **SQuAD 2.0**.

Значения EM и F1 довольно низкое (после небольшого поиска, нашел, что казуальные модели не предназначены для решения данной задачи, следовательно, низкая метрики вполне приемлима, тонкая настройка способна улучшить качество, для решения данной задачи).

Был выбран промпт, соответствующий максимальным метрикам.

Пример промпта с данными из датасета
```
You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer "No answer".
Context:
The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ("Norman" comes from "Norseman") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.
Question:
From which countries did the Norse originate?
Answer:
```

Выполним тестовые запуски для LoRA и DoRA соответственно, на датасетах меньшего размера (для теста), были получены метрики, почти сопостовимые с метриками, которые были получены при использовании выбранного промпта (речь идет о `model_promptUpdate_eval_20-05-2025_22-07-40.log`)

`model_dora_train_256_shortRun_23-05-2025_09-42-15.log` - EPOCH 1, EM: 0.28125 F1: 0.30082070707070707

`model_lora_train_256_shortRun_23-05-2025_09-42-11.log` - EPOCH 1, EM: 0.21875 F1: 0.2919890873015873

---

## Результат тонкой настройки

Тонкая настройка для LoRA не удалась, потери быстро начали падать, позже пошли вверх, что можно наблюдать в 

`model_lora_train_256_shortRun_23-05-2025_09-46-49.log`

Тонкая настройка с использованием DoRA показала лучшие результаты, даже увеличелись метрики, что можно наблюдать в `model_dora_train_256_shortRun_23-05-2025_11-30-27.log` - EM: 0.31866093262501055 F1: 0.41492821920624784

---

## Выводы

1. Даже в условиях очень грубой демонстрации, DoRA показала себя лучше, чем LoRA (не совсем корректно так утвержать, было использована конфигурация LoRA от разработчиков DoRA, возможно, использовав из HF, результат был бы лучше).
2. Процесс тонкой настройки очень сложный, стоит понять, можно ли решить данную задачу не прибегая к FT, например, подобрав соответствующий промпт.
3. Стоит выбрать модель, которая будет применяться для решения той или иной задачи (скорей всего модель Llama на 8B показала бы себя лучше, даже использовав квантованную версию)
4. Ухудшение результатов при использовании LoRA скорей всего связано с неправильной конфигурацией LoRA.
5. В рамках 1 эпохи провести качественную тонкую настройку, предположительно, не представляется возможным, но как можно заметить, даже в результате прохода 1 эпохи, качество при DoRA смогло вырасти, примерно на 10% для EM и F1 соответственно.
