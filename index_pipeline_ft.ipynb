{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304fa1ca",
   "metadata": {},
   "source": [
    "## В данном блокноте содержится pipeline для тонкой настройки Gemma 3 1B используя подход DoRA, рассмотренный в работе \"DoRA: Weight-Decomposed Low-Rank Adaptation\", [ICML](https://openreview.net/forum?id=3d5CIRG1n2) 2024.\n",
    " \n",
    "- [archive](https://arxiv.org/html/2402.09353v6) \n",
    "- [github](https://github.com/NVlabs/DoRA)\n",
    "\n",
    "PS: `index_Gemma1B.ipynb` **не актуален**. Процесс будет изложен в данном блокноте.\n",
    "\n",
    "---\n",
    "\n",
    "1. В работе исследуется применение методов PEFT, для моделей разных размеров (от 7B до 13B) и тонкой настройки на соответствующих датасетах (приведены в работе).\n",
    "Сравнение полученных результатов (таблица 1 в работе). В работе утвердается, что при использовании метода DoRA, точность модели достигается как при полной тонкой настройки, но естественно осущестлвяется эффективней, так как обновляются только адаптеры, а не все параметры, как при тонкой настройки, что подтвержается результами сравнения, приведенными в таблице 1.\n",
    "\n",
    "2. Я отойду от работы и сделаю изменения, что буду рассматривать. \n",
    "Как и в работе я проведу сравнение, только двух методов PEFT - LoRA и DoRA.\n",
    "В качестве модели я использую казуальную модель (Как и Ламы в работе) [Gemma-3 1B](https://huggingface.co/google/gemma-3-1b-it) (параметров значительно меньше), выбор чисто из соображений к вычислительным ресурсам (запросто можно влять любую другую модель и проделать то же самое).\n",
    "\n",
    "- В качестве решаемой задачи я выберу - ответ на вопрос по контексту.\n",
    "- В качестве датасета и бенчмарка использую **SQuAD 2.0** (тренировочную и валидационную части).\n",
    "- Выполню предобработку датасета для тонкой настройки (см. PSS2)\n",
    "- В качестве метрик использую **Exact Match** и **F1**.\n",
    "- Сделаю тонкую настройку на одной эпохе (чисто из прагматических соображений)\n",
    "\n",
    "3. FT\n",
    "* Проведу оценку модели на валидационной части **SQuAD 2.0** с разными промптами, температурами, приведу логи, сделаю анализ, какой промпт и температуру использовать для FT.\n",
    "* Выполню PEFT с **LoRA**, оценю на валидационной части.\n",
    "* Выполню PEFT c **DoRA**, оценю на валидационной части.\n",
    "* Сравню результаты. Сделаю выводы. \n",
    "\n",
    "\n",
    "\n",
    "**Модель загружаю с Hugging Face (необходимо получить доступ к репозиторию, модель закрыта)**\n",
    "\n",
    "**SQuAD 2.0 так же загружаю из HF**\n",
    "\n",
    "---\n",
    "- PSS: Предобработка датасета выполняется в `index_dataset_preprocess.ipynb`, в текущем блокноте только загружается\n",
    "- PSS2: В ходе работы возникла проблема, что обрабатывать последовательности на 1024 токена не получается (на этапе обучение, на инференсе все нормально), следовательно, для тонкой настройки, тренировочную часть датасета (контекст самая большая часть) обрежу, валидационную часть так же обрежу. После сделаю финальный замер метрик на (1024 токенах) (да, модель будет обучаться на последовательностях меньшей длинны, чем изначально производилась оценка, на последовательностях меньшей длинны, скорей всего модель должна показать лучие резульаты, но финальные метрик (на 1024 токена) могут упасть, потому что модель не училась на таких больших последовательностях). Плюс стоит отметить, что в **SQuAD 2.0** последовательностей ближе к 1024 не так то и много, в среднем 300 токенов должно хватить, чтобы покрыть большую часть датасета, примерная оценка на глаз, полученная из анализа датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c211c",
   "metadata": {},
   "source": [
    "### Необходимые импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7d81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# for use LoRa (HF)\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# For use Dora\n",
    "sys.path.append(os.path.join(os.getcwd(), \"peft/src/\"))\n",
    "from peft import (  # noqa: E402\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    DoraConfig,\n",
    "    BottleneckConfig,\n",
    "    PrefixTuningConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk #Загрузка датасета \n",
    "import evaluate #Для оценки модели\n",
    "\n",
    "from tqdm import tqdm #for visualization\n",
    "\n",
    "import gc\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07646ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: google/gemma-3-1b-it \n",
      "DEVICE: cuda:0\n",
      "train data: ../ft_v1/prep_datasets/train_ds_short\n",
      "val data: ../ft_v1/prep_datasets/val_ds_short\n",
      "batch_size: 2\n"
     ]
    }
   ],
   "source": [
    "# ----- | Параметры | -----\n",
    "\n",
    "OUT_DATA_CACHE = \"../ft_v1/prep_datasets\" #Директория, с предобработанными датасетами\n",
    "CACHE_DIR = \"../myDoRA_repeat/cache_dir\" #Директория с кешем модели\n",
    "EVAL_MODEL_DIR_OUT = \"./eval_models_out\" #Для вывода логов\n",
    "\n",
    "MODEL_PATH = \"google/gemma-3-1b-it\" #Название модели\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #Проверка доступности CUDA\n",
    "\n",
    "MAX_LEN_PROMPT_TOKENIZER = 256 #Длинна обрабатываемой последовательности\n",
    "MAX_LEN_LABELS_AND_NEW_TOKENS = 16 #Длинна генерируемой последовательности\n",
    " \n",
    "BATCH_SIZE = 2        #Размер батча\n",
    "BATCH_SIZE_EVAL = 2    #Размер батча для оценки модели\n",
    "\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "PADDING_SIDE_TOKENIZER = \"left\" #Добавление отступов для tokenizer\n",
    "\n",
    "PATH2DATA_TRAIN = f\"{OUT_DATA_CACHE}/train_ds_short\"\n",
    "PATH2DATA_VAL = f\"{OUT_DATA_CACHE}/val_ds_short\"\n",
    "\n",
    "# -------- Параметры для FT --------\n",
    "EPOCH = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 #Сколько батчей накапливать градиенты\n",
    "TRAIN_LOGG_STEP = 1000\n",
    "SAVE_MODEL_DIR_LORA = \"./lora_ft_squad2\"\n",
    "SAVE_MODEL_DIR_DORA = \"./dora_ft_squad2\"\n",
    "\n",
    "# -------- Параметры optimizer --------\n",
    "BETAS = (0.9, 0.99)\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"model: {MODEL_PATH} \",\n",
    "    f\"DEVICE: {DEVICE}\",\n",
    "    f\"train data: {PATH2DATA_TRAIN}\",\n",
    "    f\"val data: {PATH2DATA_VAL}\",\n",
    "    f\"batch_size: {BATCH_SIZE}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655276e3",
   "metadata": {},
   "source": [
    "### Модель Gemma-3 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbfab9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\" #Для обучения \"eager\" - более стабильный (стандартное внимание), для инференса или настройки с длинной последовательностью - \"sdpa\" (Flash Attention)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.to(DEVICE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb7cd4",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d246cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, cache_dir=CACHE_DIR)\n",
    "tokenizer.pad_token_id = 0 #Устанавливаем токен для отступа (Используется для добавления до максимальной длинны)\n",
    "tokenizer.padding_side = PADDING_SIDE_TOKENIZER #Добавлять до максимальной длинны справа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98282eb7",
   "metadata": {},
   "source": [
    "### Dataset - SQuAD 2.0 (Предобработанный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be878322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем датасет\n",
    "#Предобработанный датасет (256), но еще не токенизированный\n",
    "train_dataset = load_from_disk(f\"{OUT_DATA_CACHE}/train_ds_short\")\n",
    "val_dataset = load_from_disk(f\"{OUT_DATA_CACHE}/val_ds_short\")\n",
    "\n",
    "small_train_ds = train_dataset.select(range(32))\n",
    "small_val_ds = val_dataset.select(range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f150ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130105 11859 32 32\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(train_dataset),\n",
    "    len(val_dataset),\n",
    "    len(small_train_ds),\n",
    "    len(small_val_ds)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129ad83",
   "metadata": {},
   "source": [
    "### Шаблон промпта и для предобработки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73c96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template(context, question, answer=None):\n",
    "    #Первая инструкция, занимает 24 токена\n",
    "    # instruction = \"Answer the following question using only the information present in the provided context. Do not generate answers from outside the context.\"\n",
    "    # instruction = \"Answer the question based *only* on the context provided. If the answer cannot be found in the context, don't answer.\"\n",
    "    # instruction = 'You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".'\n",
    "    # instruction = 'You are a strict AI assistant designed to generate answers based on a given context. Your task is to generate the answer exactly as it appears in the context, using the shortest possible phrase that matches the context verbatim. Do not rephrase, add extra text, or guess. If the question asks for something not explicitly stated in the context or involves negation (e.g., \"What is not...\"), return \"No answer\" unless the context explicitly provides a negative statement.'\n",
    "    instruction = 'You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".'\n",
    "    if answer: #Если передали ответ\n",
    "        return f\"\"\"{instruction}\\nContext:\\n{context}\\nQuestion:\\n{question}\\nAnswer: {answer}\"\"\"\n",
    "    return f\"\"\"{instruction}\\nContext:\\n{context}\\nQuestion:\\n{question}\\nAnswer: \"\"\"\n",
    "\n",
    "def preprocess_datasets(examples):\n",
    "    \"\"\"\n",
    "    Принимает батч   \n",
    "    examples - словарь\n",
    "\n",
    "    return - словарь (список, список, список)\n",
    "    \"\"\"\n",
    "    contexts = examples[\"context\"] #Берем батч контекстов\n",
    "    questions = examples[\"question\"] #Беремем батч вопросов\n",
    "    # --- Без предобработки ---\n",
    "    # answers = [ans[\"text\"][0] if ans[\"text\"] else \"No answer\" for ans in examples[\"answers\"]] #Извлекаем ответы на естественном языке\n",
    "    # --- С предобработкой ---\n",
    "    answers = [\"\" if ans == \"\" else ans for ans in examples[\"answers\"]] #Извлекаем ответы на естественном языке\n",
    "\n",
    "    # Форматируем промпты\n",
    "    prompts = [\n",
    "        prompt_template(context, question) for context, question in zip(contexts, questions)\n",
    "    ]\n",
    "    \n",
    "    #Можно сделать более лучше, но пока оставим как есть\n",
    "    answers_prompts = [\n",
    "        prompt_template(context, question, answer) for context, question, answer in zip(contexts, questions, answers)\n",
    "    ]\n",
    "\n",
    "    #Токенизируем промпты `prompts`. \n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",           #Дополняем последовательность до максимальной длинны\n",
    "        truncation=True,                #Обрезать слишком длинные последовательности\n",
    "        max_length=MAX_LEN_PROMPT_TOKENIZER,   #Максимальная длинна \n",
    "        return_tensors=\"pt\"             #Возвращать как torch.tensor\n",
    "    ) #Кстати возвращает не тензоры, а списки (словарь списков)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer(): #Обеспечивает корректную токенизацию меток\n",
    "        labels = tokenizer(\n",
    "            answers_prompts,\n",
    "            padding=\"max_length\",           #Дополняем последовательность до максимальной длинны\n",
    "            truncation=True,                #Обрезать слишком длинные последовательности\n",
    "            max_length=MAX_LEN_PROMPT_TOKENIZER,   #Максимальная длинна \n",
    "            return_tensors=\"pt\"\n",
    "        ) #Кстати возвращает не тензоры, а списки (словарь списков)\n",
    "\n",
    "    inputs[\"labels\"] = labels['input_ids'] #Добавляем в словарь с тензорами еще и labels\n",
    "    inputs[\"answers\"] = answers #Добавляем ответы на естественном языке\n",
    "    \n",
    "    #Поставим, чтобы просто игнорировал - tokenizer.pad_token_id и все\n",
    "    # inputs[\"labels\"][inputs[\"labels\"] == tokenizer.pad_token_id] = -100 #Для того, чтобы токены отступа игнорировались\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d45aa",
   "metadata": {},
   "source": [
    "#### Формируем промпты и токенизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e370fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем весь датасет, по контексту не усекаем (все влезает)\n",
    "train_ds_tokenize = train_dataset.map(\n",
    "    preprocess_datasets, #Функция, которая применяется ко всем строкам\n",
    "    batched=True,        #Использовать батчинг\n",
    "    num_proc=1,         #Количество процессов\n",
    "    remove_columns=train_dataset.column_names,  #Удаляем исходные колонки\n",
    "    cache_file_name=\"./cdatasets/train_ds_full_1024.cache\" #Папка с кешом предобработанных данных\n",
    ")\n",
    "\n",
    "val_ds_tokenize = val_dataset.map(\n",
    "    preprocess_datasets, #Функция, которая применяется ко всем строкам\n",
    "    batched=True,        #Использовать батчинг\n",
    "    num_proc=1,         #Количество процессов\n",
    "    remove_columns=val_dataset.column_names,  #Удаляем исходные колонки\n",
    "    cache_file_name=\"./cdatasets/val_ds_full_1024.cache\" #Папка с кешом предобработанных данных\n",
    ")\n",
    "\n",
    "\n",
    "small_train_ds_tokenize = small_train_ds.map(\n",
    "    preprocess_datasets, #Функция, которая применяется ко всем строкам\n",
    "    batched=True,        #Использовать батчинг\n",
    "    num_proc=1,         #Количество процессов\n",
    "    remove_columns=small_train_ds.column_names,  #Удаляем исходные колонки\n",
    "    cache_file_name=\"./cdatasets/train_ds_short_1024.cache\" #Папка с кешом предобработанных данных\n",
    ")\n",
    "\n",
    "small_val_ds_tokenize = small_val_ds.map(\n",
    "    preprocess_datasets, #Функция, которая применяется ко всем строкам\n",
    "    batched=True,        #Использовать батчинг\n",
    "    num_proc=1,         #Количество процессов\n",
    "    remove_columns=small_val_ds.column_names,  #Удаляем исходные колонки\n",
    "    cache_file_name=\"./cdatasets/val_ds_short_1024.cache\" #Папка с кешом предобработанных данных\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb478f5-6714-454f-86c3-d42bffe7b3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".\\nContext:\\nBeyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time.\\nQuestion:\\nWhen did Beyonce start becoming popular?\\nAnswer: '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([train_ds_tokenize[0]['input_ids']],skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc7cf03-7f12-45e7-9dd0-ffcdb5f80e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".\\nContext:\\nBeyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time.\\nQuestion:\\nWhen did Beyonce start becoming popular?\\nAnswer: ']\n",
      "['You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".\\nContext:\\nThe Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.\\nQuestion:\\nIn what country is Normandy located?\\nAnswer: ']\n",
      "['You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".\\nContext:\\nBeyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time.\\nQuestion:\\nWhen did Beyonce start becoming popular?\\nAnswer: ']\n",
      "['You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".\\nContext:\\nThe Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.\\nQuestion:\\nIn what country is Normandy located?\\nAnswer: ']\n"
     ]
    }
   ],
   "source": [
    "# Check tokenized data\n",
    "print(tokenizer.batch_decode([train_ds_tokenize[0]['input_ids']], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode([val_ds_tokenize[0]['input_ids']], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode([small_train_ds_tokenize[0]['input_ids']], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode([small_val_ds_tokenize[0]['input_ids']], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29164f94",
   "metadata": {},
   "source": [
    "#### Функция для формирования батчей (при загрузке из Dataloader'a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929b8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    ''' \n",
    "    batch - список словарей ....\n",
    "        {\"input_ids\": [...], \"attention_mask\": [...], \"answers\": \"...\"},\n",
    "        {\"input_ids\": [...], \"attention_mask\": [...], \"answers\": \"...\"},\n",
    "    '''\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    answers = [item['answers'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids),\n",
    "        \"attention_mask\": torch.stack(attention_mask),\n",
    "        \"labels\": torch.stack(labels),\n",
    "        \"answers\": answers\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388b809",
   "metadata": {},
   "source": [
    "#### Dataloader's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "617db91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65053 5930 16 16\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_ds_tokenize,                # Датасет (например, tokenized_dataset)\n",
    "    batch_size=BATCH_SIZE,         # Размер батча\n",
    "    shuffle=False,         # Перемешивать данные\n",
    "    num_workers=0,         # Количество потоков для загрузки\n",
    "    collate_fn=custom_collate_fn,       # Функция для сборки батча\n",
    "    pin_memory=False,      # Копировать данные в CUDA-память\n",
    "    drop_last=False,       # Отбрасывать последний неполный батч\n",
    "    prefetch_factor=None,     # Количество батчей для предварительной загрузки (сколько батчей будет загружено сразу для  ускорения, только в параллельном режиме (когда num_workers > 0)) (None - если не используем)\n",
    "    persistent_workers=False  # Сохранять рабочие потоки между итерациями\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_ds_tokenize,                # Датасет (например, tokenized_dataset)\n",
    "    batch_size=BATCH_SIZE_EVAL,         # Размер батча\n",
    "    shuffle=False,         # Перемешивать данные\n",
    "    num_workers=0,         # Количество потоков для загрузки\n",
    "    collate_fn=custom_collate_fn,       # Функция для сборки батча\n",
    "    pin_memory=False,      # Копировать данные в CUDA-память\n",
    "    drop_last=False,       # Отбрасывать последний неполный батч\n",
    "    prefetch_factor=None,     # Количество батчей для предварительной загрузки (сколько батчей будет загружено сразу для  ускорения, только в параллельном режиме (когда num_workers > 0)) (None - если не используем)\n",
    "    persistent_workers=False  # Сохранять рабочие потоки между итерациями\n",
    ")\n",
    "\n",
    "\n",
    "small_train_ds_tokenize_dataloader = DataLoader(\n",
    "    small_train_ds_tokenize,                # Датасет (например, tokenized_dataset)\n",
    "    batch_size=BATCH_SIZE_EVAL,         # Размер батча\n",
    "    shuffle=False,         # Перемешивать данные\n",
    "    num_workers=0,         # Количество потоков для загрузки\n",
    "    collate_fn=custom_collate_fn,       # Функция для сборки батча\n",
    "    pin_memory=False,      # Копировать данные в CUDA-память\n",
    "    drop_last=False,       # Отбрасывать последний неполный батч\n",
    "    prefetch_factor=None,     # Количество батчей для предварительной загрузки (сколько батчей будет загружено сразу для  ускорения, только в параллельном режиме (когда num_workers > 0)) (None - если не используем)\n",
    "    persistent_workers=False  # Сохранять рабочие потоки между итерациями\n",
    ")\n",
    "\n",
    "small_val_ds_tokenize_dataloader = DataLoader(\n",
    "    small_val_ds_tokenize,                # Датасет (например, tokenized_dataset)\n",
    "    batch_size=BATCH_SIZE_EVAL,         # Размер батча\n",
    "    shuffle=False,         # Перемешивать данные\n",
    "    num_workers=0,         # Количество потоков для загрузки\n",
    "    collate_fn=custom_collate_fn,       # Функция для сборки батча\n",
    "    pin_memory=False,      # Копировать данные в CUDA-память\n",
    "    drop_last=False,       # Отбрасывать последний неполный батч\n",
    "    prefetch_factor=None,     # Количество батчей для предварительной загрузки (сколько батчей будет загружено сразу для  ускорения, только в параллельном режиме (когда num_workers > 0)) (None - если не используем)\n",
    "    persistent_workers=False  # Сохранять рабочие потоки между итерациями\n",
    ")\n",
    "\n",
    "print(\n",
    "    len(train_dataloader), #Количество батчей\n",
    "    len(val_dataloader), #Количество батчей\n",
    "    len(small_train_ds_tokenize_dataloader),\n",
    "    len(small_val_ds_tokenize_dataloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321745e0",
   "metadata": {},
   "source": [
    "### Функции для оценки ответов (нормализация текста, EM, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4fbb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Нормализует текст: нижний регистр, удаление пробелов и пунктуации\n",
    "    \"\"\"\n",
    "    text = text.lower() # Приводим к нижнему регистру\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # Удаляем пунктуацию\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Удаляем лишние пробелы\n",
    "    return text\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Вычисляет Exact Match для одного примера\n",
    "    Принимает 2 строки для сравнения\n",
    "    \"\"\"\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1_score(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Вычисляет F1 Score для одного примера\n",
    "    Принимает 2 строки для сравнения\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 and len(truth_tokens) == 0: # Если оба ответа пустые, F1 = 1\n",
    "        return 1.0\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0: # Если один из ответов пустой, F1 = 0\n",
    "        return 0.0\n",
    "    \n",
    "    # Находим общие токены\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    tp = len(common_tokens)\n",
    "    \n",
    "    precision = tp / len(pred_tokens)  # Precision = TP / (TP + FP), где FP = предсказанные токены, не входящие в правильные\n",
    "    recall = tp / len(truth_tokens) # Recall = TP / (TP + FN), где FN = правильные токены, не входящие в предсказанные\n",
    "    \n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1700229",
   "metadata": {},
   "source": [
    "#### Вычисление средних метрик по всему датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48e155a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluate_metrics_EM_F1(em_all, f1_all):\n",
    "    ''' \n",
    "    Функция для вычисления среднего значения EM и F1\n",
    "    '''\n",
    "    em_all = np.array(em_all)\n",
    "    f1_all = np.array(f1_all)\n",
    "    print(f\"EM: {em_all.mean()}, F1: {f1_all.mean()}\")\n",
    "    return em_all.mean(), f1_all.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c3e65",
   "metadata": {},
   "source": [
    "### Функция для оценки ответов модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7c287",
   "metadata": {},
   "source": [
    "* Расчет метрик от HF закоментированны намеренно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc8ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_ds(model, tokenizer, dataloader, logger = None, split_seq = \"\\nAnswer: \"):\n",
    "\n",
    "    em_all = []\n",
    "    f1_all = []\n",
    "\n",
    "    # ---- Для вычисления метрик от HF ----\n",
    "    # pred = []\n",
    "    # ref = []\n",
    "\n",
    "    cnt_index = 0\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        inputs_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels'] #Только для обучения \n",
    "        answers = batch['answers'] #Список ответов на естественном языке\n",
    "\n",
    "        # ---- Перенести тензоры на DEVICE ----\n",
    "        # ---- Генерация ответа моделью ----\n",
    "        with torch.no_grad(): #Отключить накопление градиентов (во время инференса)\n",
    "            output_model = model.generate(\n",
    "                input_ids=inputs_ids.to(DEVICE),\n",
    "                attention_mask=attention_mask.to(DEVICE),\n",
    "                max_new_tokens=MAX_LEN_LABELS_AND_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE\n",
    "            ) #tensor: input_ids - (batch_size, max_len + max_new_tokens)\n",
    "        \n",
    "        # ---- Декодирование ----\n",
    "        text_output_model = tokenizer.batch_decode(output_model, skip_special_tokens=True)\n",
    "        #  ---- Декодирование исходной последовательности ----\n",
    "        input_decode = tokenizer.batch_decode(inputs_ids, skip_special_tokens=True)\n",
    "\n",
    "        # ---- Пробегаемся по ответам модели и вычисляем метрики ----\n",
    "        for text_item, answr, inp_seq in zip(text_output_model, answers, input_decode):\n",
    "            answer_model_text = \"\"\n",
    "            text_split = text_item.split(split_seq)\n",
    "            # if len(text_split) > 1:\n",
    "            answer_model_text = text_split[-1].strip()\n",
    "            answer_model_text = \"\" if answer_model_text == \"No answer\" else answer_model_text\n",
    "\n",
    "            em_item = compute_exact_match(answer_model_text, answr)\n",
    "            f1_item = compute_f1_score(answer_model_text, answr)\n",
    "\n",
    "            em_all.append(em_item)\n",
    "            f1_all.append(f1_item)\n",
    "            \n",
    "            # ---- Формирование списков для вычисления метрик от HF ----\n",
    "            # pred.append({\"id\" : f\"{cnt_index}\", \"prediction_text\" : answer_model_text, \"no_answer_probability\" : 1.0 if answer_model_text == \"\" else 0.0})\n",
    "            # ref.append({\"id\": f\"{cnt_index}\", \"answers\": {\"text\": [answr], \"answer_start\": []}})\n",
    "            # cnt_index += 1\n",
    "\n",
    "            if logger:\n",
    "                logger.info(f\"-----\\n{inp_seq}\\n[ANSW]:{answr}\\n[MODL]:{answer_model_text}\\nEM:{em_item}\\nF1:{f1_item}\\n-----\")\n",
    "        \n",
    "        # ---- Очистка памяти ----\n",
    "        del inputs_ids, attention_mask, answers, output_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # ---- Вычисление метрик HF ----\n",
    "    # hg_res = squad_metric.compute(predictions=pred, references=ref)\n",
    "    # if logger:\n",
    "    #     logger.info(f\"HF EM: {hg_res['exact']} F1: {hg_res['f1']}\")\n",
    "    \n",
    "    return em_all, f1_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d63c682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No answer'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh = \"     \\n \\n \\n No answer    \"\n",
    "hh = hh.strip()\n",
    "hh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258c8bb",
   "metadata": {},
   "source": [
    "### Для тонкой настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66cb1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для вычисления потерь на validation\n",
    "def evaluate(model, dataloader):\n",
    "    ''' \n",
    "    Вычисляем loss на валидационном датасете \n",
    "    dataloader : tqdm\n",
    "    '''\n",
    "    model.attn_implementation=\"sdpa\"\n",
    "    model.eval() #Переводим модель в режим оценки\n",
    "    total_loss_eval = 0\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        total_loss_eval += outputs.loss.item()\n",
    "       \n",
    "        #Чистим: outputs, input_ids, attention_mask, labels для сохранения памяти\n",
    "        del outputs, input_ids, attention_mask, labels\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return total_loss_eval / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02681495",
   "metadata": {},
   "source": [
    "#### Функция для FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db66085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(model, dataloader, dataloader_val, optimizer, epochs = 1, accumulation_steps = 8, logger = None, train_logg_step = 100, dir_save_model = \"./default_ft\"):\n",
    "    ''' Функция для тренировки модели '''\n",
    "    GLOBAL_STEP = 0\n",
    "    TOTAL_LOSS = 0\n",
    "    loss_for_logging = 0 \n",
    "    steps_for_logging = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tqdm_dataloader = tqdm(dataloader, desc=f\"Train EPOCH: {epoch+1}\")\n",
    "        total_loss_epoch = 0\n",
    "                        \n",
    "        model.attn_implementation=\"eager\"\n",
    "        model.train()\n",
    "        \n",
    "        # Очищаем градиенты до захода в цикл\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for idx, batch in enumerate(tqdm_dataloader): #Идем по даталоадеру\n",
    "            #Распаковываем batch\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask =  batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            #Переносим на устройство\n",
    "            input_ids, attention_mask, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Размеры input_ids и labels должны совпадать, для вычисления loss.\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss #Получаем потери\n",
    "            loss = loss / accumulation_steps #Нормализация для накопления\n",
    "            loss.backward() #Обратное распространение\n",
    "\n",
    "            # Сохраняем потерю\n",
    "            batch_loss = loss.item() * accumulation_steps\n",
    "            total_loss_epoch += batch_loss\n",
    "            TOTAL_LOSS += batch_loss\n",
    "            loss_for_logging += batch_loss\n",
    "            steps_for_logging += 1\n",
    "\n",
    "            #Шаг оптимизатора только после накопления градиентов\n",
    "            if (idx + 1) % accumulation_steps == 0 or (idx + 1) == len(tqdm_dataloader):\n",
    "                optimizer.step() #Обновление параметров (Градиенты накапливаются до вызова optimizer.step())\n",
    "                optimizer.zero_grad()  ## Очищаем градиенты после шага оптимизатора\n",
    "\n",
    "            if GLOBAL_STEP > 0 and GLOBAL_STEP % train_logg_step == 0:\n",
    "                avg_loss = loss_for_logging / steps_for_logging\n",
    "                if logger: \n",
    "                    logger.info(f\"Step {GLOBAL_STEP}, Avg Train Loss (last {train_logg_step} steps): {avg_loss:.4f}\")\n",
    "                print(f\"Step {GLOBAL_STEP}, Avg Train Loss (last {train_logg_step} steps): {avg_loss:.4f}\")\n",
    "                # Сбрасываем накопленные значения\n",
    "                loss_for_logging = 0\n",
    "                steps_for_logging = 0\n",
    "            \n",
    "            GLOBAL_STEP += 1\n",
    "\n",
    "            #Чистим: outputs, input_ids, attention_mask, labels для сохранения памяти\n",
    "            del outputs, input_ids, attention_mask, labels, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        #Средняя потеря за эпоху\n",
    "        avg_loss_epoch = total_loss_epoch / len(tqdm_dataloader)\n",
    "        if logger:\n",
    "            logger.info(f\"EPOCH {epoch+1}, AVG TRAIN LOSS: {avg_loss_epoch:.4f}\")\n",
    "        print(f\"EPOCH {epoch+1}, AVG TRAIN LOSS: {avg_loss_epoch:.4f}\")\n",
    "\n",
    "        #Оценка на валидационной выборке\n",
    "        tqdm_dataloader_val = tqdm(dataloader_val, desc=f\"Val EPOCH: {epoch+1}\")\n",
    "        val_loss = evaluate(model, tqdm_dataloader_val)\n",
    "        if logger:\n",
    "            logger.info(f\"EPOCH {epoch+1}, Val loss: {val_loss:.4f}\")\n",
    "        print(f\"EPOCH {epoch+1}, Val loss: {val_loss:.4f}\")\n",
    "\n",
    "        #Оценка метрик EM и F1 (На валидационном датасете)\n",
    "        em_all, f1_all = eval_model_ds(model, tokenizer, dataloader_val, logger=logger)\n",
    "        em_r, f1_r = calculate_evaluate_metrics_EM_F1(em_all, f1_all)\n",
    "        if logger:\n",
    "            logger.info(f\"EPOCH {epoch+1}, EM: {em_r} F1: {f1_r}\")\n",
    "        print(f\"EPOCH {epoch+1}, EM: {em_r} F1: {f1_r}\")\n",
    "\n",
    "        # Сохранение модели (опционально)\n",
    "        model.save_pretrained(f\"{dir_save_model}/model_epoch_{epoch+1}\")\n",
    "        tokenizer.save_pretrained(f\"{dir_save_model}/model_epoch_{epoch+1}\")\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\"Training completed!\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cf635",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44581e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Параметры для LORA --------\n",
    "LORA_R = 32                       # Ранг LoRA (Размер матриц адаптации)\n",
    "lora_alpha = 2*LORA_R                  # Масштабирующий коэффициент (Уселение эффекта адаптации)\n",
    "target_modules = [\"q_proj\", \"v_proj\"]   # Модули для LoRA (Применяется к слоям внимания)\n",
    "#q_proj, k_proj, v_proj, o_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9b53aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- LORA Config --------\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,                       # Ранг LoRA\n",
    "    lora_alpha=lora_alpha,          # Масштабирование\n",
    "    target_modules=target_modules,  # Целевые модули\n",
    "    lora_dropout=0.05,              # Dropout для регуляризации\n",
    "    bias=\"none\",                    # Без смещений в LoRA\n",
    "    task_type=\"CAUSAL_LM\",          # Тип задачи\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f38ce82-00ac-4c93-9b9a-a911e5a9b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df94fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2,981,888\n"
     ]
    }
   ],
   "source": [
    "#Сколько параметров будет обучаться при LoRA\n",
    "trainable_params_LoRa = sum(p.numel() for p in LoRA_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params_LoRa:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486538e3",
   "metadata": {},
   "source": [
    "### Optimizer for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(LoRA_model.parameters(), lr=LEARNING_RATE, betas=BETAS, eps=EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced7bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Создаем файл логга ---\n",
    "timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "log_filename = f\"{EVAL_MODEL_DIR_OUT}/model_train_lora_256_{timestamp}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Уровень логирования\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Формат сообщений\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding='utf-8'),  # Запись в файл\n",
    "        # logging.StreamHandler()  # Вывод в консоль (опционально)\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_logger = logging.getLogger()\n",
    "\n",
    "my_logger.info(f\"\"\"\n",
    "Title: This step train model with use LoRA\n",
    "Process: Train model\n",
    "\n",
    "--- Parameters Training ---\n",
    "MODEL: {MODEL_PATH}\n",
    "DEVICE: {DEVICE}\n",
    "max_len_tokens: {MAX_LEN_PROMPT_TOKENIZER}\n",
    "new_max_tokens: {MAX_LEN_LABELS_AND_NEW_TOKENS}\n",
    "TEMPERATURE: {TEMPERATURE}\n",
    "\n",
    "BATCH_SIZE: {BATCH_SIZE}\n",
    "BATCH_SIZE_EVAL: {BATCH_SIZE_EVAL}\n",
    "\n",
    "PADDING_SIDE_TOKENIZER: {PADDING_SIDE_TOKENIZER}\n",
    "\n",
    "OUT_DATA_CACHE: {OUT_DATA_CACHE}\n",
    "CACHE_DIR: {CACHE_DIR}\n",
    "EVAL_MODEL_DIR_OUT: {EVAL_MODEL_DIR_OUT}\n",
    "\n",
    "Train data: {PATH2DATA_TRAIN}\n",
    "Val data: {PATH2DATA_VAL}\n",
    "\n",
    "--- Parameters for LORA ---\n",
    "TRAINED PARAMETERS: {trainable_params_LoRa}\n",
    "\n",
    "EPOCH: {EPOCH}\n",
    "LEARNING_RATE: {LEARNING_RATE}\n",
    "GRADIENT_ACCUMULATION_STEPS: {GRADIENT_ACCUMULATION_STEPS}\n",
    "TRAIN_LOGG_STEP: {TRAIN_LOGG_STEP}\n",
    "SAVE_MODEL_DIR_LORA: {SAVE_MODEL_DIR_LORA}\n",
    "SAVE_MODEL_DIR_DORA: {SAVE_MODEL_DIR_DORA}\n",
    "\n",
    "--- Parameters for optimizer ---\n",
    "BETAS: {BETAS}\n",
    "EPS: {EPS}\n",
    "\n",
    "--- Model architecture ---\n",
    "{str(LoRA_model)}\n",
    "\n",
    "--- Model config ---\n",
    "{LoRA_model.config}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2ddf8",
   "metadata": {},
   "source": [
    "### Запуск для LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d6e0de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train EPOCH: 1:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train EPOCH: 1: 100%|██████████| 16/16 [00:26<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1, AVG TRAIN LOSS: 13.8542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val EPOCH: 1: 100%|██████████| 16/16 [00:07<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1, Val loss: 10.9801\n",
      "EM: 0.25, F1: 0.319612678987679\n",
      "EPOCH 1, EM: 0.25 F1: 0.319612678987679\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Full\n",
    "# my_train(\n",
    "#     LoRA_model, \n",
    "#     train_dataloader, \n",
    "#     val_dataloader, \n",
    "#     optimizer, \n",
    "#     epochs = EPOCH, \n",
    "#     accumulation_steps = GRADIENT_ACCUMULATION_STEPS, \n",
    "#     logger = my_logger, \n",
    "#     train_logg_step = TRAIN_LOGG_STEP, \n",
    "#     dir_save_model = SAVE_MODEL_DIR_LORA\n",
    "# )\n",
    "\n",
    "# Short \n",
    "my_train(\n",
    "    LoRA_model, \n",
    "    small_train_ds_tokenize_dataloader, \n",
    "    small_val_ds_tokenize_dataloader, \n",
    "    optimizer, \n",
    "    epochs = EPOCH, \n",
    "    accumulation_steps = GRADIENT_ACCUMULATION_STEPS, \n",
    "    logger = my_logger, \n",
    "    train_logg_step = TRAIN_LOGG_STEP, \n",
    "    dir_save_model = SAVE_MODEL_DIR_LORA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe99c70",
   "metadata": {},
   "source": [
    "Потеря на обучении начала расти, возможно только в рамках одной эпохе, дальше пошла бы на спад, касаемое потери на ваидации, нет возможности определить будет расти или падать, так как запускается раз в эпоху. Низкие значения метрики EM и F1 - изменил промпт, а в функции обрезки ответа модели для оценки не изменил, следовательно, функция работала некорректно, что можно заметить, по очень низким метриками (в логах так же можно это заметить).\n",
    "\n",
    "Даже если запустить в качестве теста на маленьких датасетах, F1 сохраняется на уровня 0.1 - 0.2.  \n",
    "\n",
    "Стоило пофиксить функцию оценки, как на коротких датасетах (выборка первых 32 элементов) метрики подрасли до 0.25 и 0.31 EM and F1 соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab56514",
   "metadata": {},
   "source": [
    "### Перед DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e20a082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del LoRA_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8128aa0",
   "metadata": {},
   "source": [
    "### DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06060b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Параметры для DORA --------\n",
    "dora_config = DoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,  # Дропаут\n",
    "    bias=\"none\",  # Без смещения\n",
    "    task_type=\"CAUSAL_LM\",  # Тип задачи\n",
    "    dora_simple=True,  # Упрощённый режим DoRA (Если False, норма вычисляется без .detach(), что позволяет градиентам распространяться).\n",
    "    Wdecompose_target_modules=[\"q_proj\", \"v_proj\"]  # Модули для декомпозиции весов\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "187ff7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.0.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.0.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.0.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.0.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.0.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.1.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.1.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.1.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.1.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.1.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.1.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.2.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.2.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.2.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.2.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.2.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.2.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.3.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.3.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.3.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.3.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.3.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.3.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.4.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.4.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.4.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.4.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.4.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.4.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.5.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.5.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.5.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.5.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.5.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.5.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.6.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.6.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.6.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.6.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.6.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.6.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.7.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.7.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.7.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.7.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.7.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.7.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.8.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.8.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.8.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.8.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.8.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.8.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.9.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.9.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.9.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.9.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.9.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.9.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.10.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.10.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.10.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.10.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.10.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.10.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.11.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.11.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.11.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.11.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.11.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.11.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.12.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.12.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.12.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.12.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.12.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.12.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.13.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.13.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.13.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.13.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.13.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.13.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.14.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.14.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.14.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.14.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.14.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.14.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.15.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.15.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.15.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.15.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.15.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.15.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.16.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.16.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.16.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.16.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.16.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.16.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.17.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.17.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.17.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.17.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.17.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.17.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.18.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.18.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.18.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.18.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.18.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.18.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.19.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.19.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.19.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.19.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.19.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.19.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.20.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.20.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.20.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.20.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.20.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.20.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.21.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.21.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.21.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.21.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.21.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.21.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.22.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.22.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.22.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.22.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.22.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.22.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.23.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.23.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.23.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.23.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.23.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.23.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.24.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.24.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.24.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.24.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.24.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.24.self_attn.v_proj.lora_B.weight is trainable\n",
      "model.layers.25.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.25.self_attn.q_proj.lora_A.weight is trainable\n",
      "model.layers.25.self_attn.q_proj.lora_B.weight is trainable\n",
      "model.layers.25.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "model.layers.25.self_attn.v_proj.lora_A.weight is trainable\n",
      "model.layers.25.self_attn.v_proj.lora_B.weight is trainable\n"
     ]
    }
   ],
   "source": [
    "# Добавить адаптеры к исходной модели (Не спутать с моделью, после применения конфига на прошлом шаге) \n",
    "DoRA_model = get_peft_model(model, dora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57c863-5702-4c36-9679-58576d79797d",
   "metadata": {},
   "source": [
    "### optimizer for DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61eeedc-2dd8-483e-a496-70ee1b157d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerDora = AdamW(DoRA_model.parameters(), lr=LEARNING_RATE, betas=BETAS, eps=EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2778c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 3,015,168\n"
     ]
    }
   ],
   "source": [
    "#Сколько параметров будет обучаться при DoRA\n",
    "trainable_params_Dora = sum(p.numel() for p in DoRA_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params_Dora:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb006654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Создаем файл логга ---\n",
    "timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "log_filename = f\"{EVAL_MODEL_DIR_OUT}/model_train_dora_256_{timestamp}.log\"\n",
    "\n",
    "logging.getLogger().handlers.clear() #Очистка предыдущего\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Уровень логирования\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Формат сообщений\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding='utf-8'),  # Запись в файл\n",
    "        # logging.StreamHandler()  # Вывод в консоль (опционально)\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_logger2 = logging.getLogger()\n",
    "\n",
    "my_logger2.info(f\"\"\"\n",
    "Title: This step train model with use DoRA\n",
    "Process: Train model\n",
    "\n",
    "--- Parameters Training ---\n",
    "MODEL: {MODEL_PATH}\n",
    "DEVICE: {DEVICE}\n",
    "max_len_tokens: {MAX_LEN_PROMPT_TOKENIZER}\n",
    "new_max_tokens: {MAX_LEN_LABELS_AND_NEW_TOKENS}\n",
    "TEMPERATURE: {TEMPERATURE}\n",
    "\n",
    "BATCH_SIZE: {BATCH_SIZE}\n",
    "BATCH_SIZE_EVAL: {BATCH_SIZE_EVAL}\n",
    "\n",
    "PADDING_SIDE_TOKENIZER: {PADDING_SIDE_TOKENIZER}\n",
    "\n",
    "OUT_DATA_CACHE: {OUT_DATA_CACHE}\n",
    "CACHE_DIR: {CACHE_DIR}\n",
    "EVAL_MODEL_DIR_OUT: {EVAL_MODEL_DIR_OUT}\n",
    "\n",
    "Train data: {PATH2DATA_TRAIN}\n",
    "Val data: {PATH2DATA_VAL}\n",
    "\n",
    "--- Parameters for DORA ---\n",
    "TRAINED PARAMETERS: {trainable_params_Dora}\n",
    "\n",
    "EPOCH: {EPOCH}\n",
    "LEARNING_RATE: {LEARNING_RATE}\n",
    "GRADIENT_ACCUMULATION_STEPS: {GRADIENT_ACCUMULATION_STEPS}\n",
    "TRAIN_LOGG_STEP: {TRAIN_LOGG_STEP}\n",
    "SAVE_MODEL_DIR_LORA: {SAVE_MODEL_DIR_LORA}\n",
    "SAVE_MODEL_DIR_DORA: {SAVE_MODEL_DIR_DORA}\n",
    "\n",
    "--- Parameters for optimizer ---\n",
    "BETAS: {BETAS}\n",
    "EPS: {EPS}\n",
    "\n",
    "--- Model architecture ---\n",
    "{str(DoRA_model)}\n",
    "\n",
    "--- Model config ---\n",
    "{DoRA_model.config}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1e0f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train EPOCH: 1: 100%|██████████| 16/16 [00:25<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1, AVG TRAIN LOSS: 13.7965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val EPOCH: 1: 100%|██████████| 16/16 [00:07<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1, Val loss: 10.9331\n",
      "EM: 0.25, F1: 0.3300007284382284\n",
      "EPOCH 1, EM: 0.25 F1: 0.3300007284382284\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Full\n",
    "# my_train(\n",
    "#     DoRA_model, \n",
    "#     train_dataloader, \n",
    "#     val_dataloader, \n",
    "#     optimizer, \n",
    "#     epochs = EPOCH, \n",
    "#     accumulation_steps = GRADIENT_ACCUMULATION_STEPS, \n",
    "#     logger = my_logger2, \n",
    "#     train_logg_step = TRAIN_LOGG_STEP, \n",
    "#     dir_save_model = SAVE_MODEL_DIR_DORA\n",
    "# )\n",
    "\n",
    "# Short \n",
    "my_train(\n",
    "    DoRA_model, \n",
    "    small_train_ds_tokenize_dataloader, \n",
    "    small_val_ds_tokenize_dataloader, \n",
    "    optimizerDora, \n",
    "    epochs = EPOCH, \n",
    "    accumulation_steps = GRADIENT_ACCUMULATION_STEPS, \n",
    "    logger = my_logger2, \n",
    "    train_logg_step = TRAIN_LOGG_STEP, \n",
    "    dir_save_model = SAVE_MODEL_DIR_DORA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b73ad4",
   "metadata": {},
   "source": [
    "Про увеличение метрик EM и F1 можно сказать и тут. То есть проблема заключалась в расчете метрик, а не в самой модели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
