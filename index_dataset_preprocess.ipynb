{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eddd6a7",
   "metadata": {},
   "source": [
    "## В данном блокноте выполняется предобработка датасета\n",
    "\n",
    "Сокращение длинны контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c03c2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/kne.21@free.uni-dubna.ru/Desktop/prog/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: HTTP Error 403: Forbidden\n",
      "[nltk_data] Error loading punkt_tab: HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk #Загрузка датасета \n",
    "\n",
    "# ----------- for ds preprocess -----------\n",
    "import nltk\n",
    "nltk.data.path.clear()\n",
    "nltk.data.path.append('/home/kne.21@free.uni-dubna.ru/Desktop/prog/nltk_data')\n",
    "nltk.download('punkt', download_dir=\"/home/kne.21@free.uni-dubna.ru/Desktop/prog/nltk_data\")\n",
    "nltk.download('punkt_tab', download_dir=\"/home/kne.21@free.uni-dubna.ru/Desktop/prog/nltk_data\")\n",
    "\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95c5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- | Параметры | -----\n",
    "\n",
    "OUT_DATA_CACHE = \"./prep_datasets\" #Директория, с предобработанными датасетами\n",
    "CACHE_DIR = \"../myDoRA_repeat/cache_dir\" #Директория с кешем модели\n",
    "EVAL_MODEL_DIR_OUT = \"./eval_models_out\" #Для вывода логов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773dffe",
   "metadata": {},
   "source": [
    "### Dataset - SQuAD 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8accb896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rajpurkar/squad_v2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'squad_v2' at cdatasets/rajpurkar___squad_v2/squad_v2/0.0.0/3ffb306f725f7d2ce8394bc1873b24868140c412 (last modified on Sat May 17 12:20:11 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='squad_v2', config_name='squad_v2', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=116742930, num_examples=130319, shard_lengths=None, dataset_name='squad_v2'), 'validation': SplitInfo(name='validation', num_bytes=11663635, num_examples=11873, shard_lengths=None, dataset_name='squad_v2')}, download_checksums={'hf://datasets/rajpurkar/squad_v2@3ffb306f725f7d2ce8394bc1873b24868140c412/squad_v2/train-00000-of-00001.parquet': {'num_bytes': 16369982, 'checksum': None}, 'hf://datasets/rajpurkar/squad_v2@3ffb306f725f7d2ce8394bc1873b24868140c412/squad_v2/validation-00000-of-00001.parquet': {'num_bytes': 1350511, 'checksum': None}}, download_size=17720493, post_processing_size=None, dataset_size=128406565, size_in_bytes=146127058)\n",
      "DatasetInfo(description='', citation='', homepage='', license='', features={'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='squad_v2', config_name='squad_v2', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=116742930, num_examples=130319, shard_lengths=None, dataset_name='squad_v2'), 'validation': SplitInfo(name='validation', num_bytes=11663635, num_examples=11873, shard_lengths=None, dataset_name='squad_v2')}, download_checksums={'hf://datasets/rajpurkar/squad_v2@3ffb306f725f7d2ce8394bc1873b24868140c412/squad_v2/train-00000-of-00001.parquet': {'num_bytes': 16369982, 'checksum': None}, 'hf://datasets/rajpurkar/squad_v2@3ffb306f725f7d2ce8394bc1873b24868140c412/squad_v2/validation-00000-of-00001.parquet': {'num_bytes': 1350511, 'checksum': None}}, download_size=17720493, post_processing_size=None, dataset_size=128406565, size_in_bytes=146127058)\n"
     ]
    }
   ],
   "source": [
    "#Загружаем датасет\n",
    "datasetSQUAD2 = load_dataset(\"rajpurkar/squad_v2\", cache_dir=\"./cdatasets\")\n",
    "\n",
    "val_dataset = datasetSQUAD2['validation'] \n",
    "train_dataset = datasetSQUAD2['train']\n",
    "\n",
    "#for test pipeline\n",
    "small_train_ds = train_dataset.select(range(32))\n",
    "small_val_ds = val_dataset.select(range(32))\n",
    "\n",
    "print(val_dataset.info)\n",
    "print(train_dataset.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ecc49",
   "metadata": {},
   "source": [
    "### Предобработка датасета \n",
    "\n",
    "Тестировал прямой проход модели на батче из 1 последовательности (1024 токена), работала, но видно с обратным распространением и шагом оптимизатора, памяти не хватает, следовательно,нужно сократить длинну последовательности до 256-512 токенов.\n",
    "\n",
    "Возьму примерно 256 токенов (контекст обрезается по предложениям, берется текущее предыдушее и следущее).\n",
    "\n",
    "Инференс на 1024 токена работает хорошо (причем я указываю что использую внимани \"eager\" для FT и \"sdpa\" для инференса, но не уверен, что оно корректно используется, не до конца протестировал)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cae191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_split_context(batch, neighbors = 1):\n",
    "    \"\"\" Функция для обработки датасета SQuAD 2.0\n",
    "    Для извлечения context, question, answer. И обрезки context до длинны 4-5 предложений.\n",
    "    \n",
    "    Статический срез, берет предыдушее предложение, текущее и следущее (index - neighbors : index + neighbors + 1)\n",
    "\n",
    "    Args:\n",
    "        batch : dict - словарь списков с соответствующими ключами dataset'a\n",
    "    \"\"\"\n",
    "    #Есть строки, в которых нет ответов (в контексте не предоставлены ответы)\n",
    "    #Необходимо, чтобы все ровно соответстовали тому, что было\n",
    "    contexts = [] #Список контекстов\n",
    "    questions = [] #Список воспросов\n",
    "    answers = [] #Список ответов\n",
    "\n",
    "    for cntxt, quest, answer in zip(batch['context'], batch['question'], batch['answers']):\n",
    "        cntx_split_sentenses = nltk.sent_tokenize(cntxt, language='english') #Разбиваем контест по предложениям\n",
    "\n",
    "        if len(answer['text']) > 0: #Ответ есть\n",
    "            found = False\n",
    "            for sent_index in range(len(cntx_split_sentenses)):\n",
    "                if (answer['text'][0] in cntx_split_sentenses[sent_index]): #Если ответ содержится в предложении\n",
    "                    start = max(0, sent_index - neighbors)\n",
    "                    end = min(len(cntx_split_sentenses), sent_index + neighbors + 1)\n",
    "                    res_text_context = \" \".join(cntx_split_sentenses[start : end])\n",
    "\n",
    "                    contexts.append(res_text_context)\n",
    "                    questions.append(quest)\n",
    "                    answers.append(answer['text'][0]) #Добавляем именно текст\n",
    "                    found = True\n",
    "                    break #ОБЯЗАТЕЛЬНО НУЖНО ВЫЙТИ из цикла\n",
    "            if not found:\n",
    "                contexts.append(\"\") #Если не нашли, будет пустой контекст (не будем использовать для обучения и оценки)\n",
    "                questions.append(quest)\n",
    "                answers.append(answer['text'][0]) #Добавляем именно текст\n",
    "        else:\n",
    "            #Нужно добавить контекст как строку, а не как список ;D\n",
    "            contexts.append(\" \".join(cntx_split_sentenses[:3])) #Берем 3 первых предложения\n",
    "            questions.append(quest)\n",
    "            answers.append(\"\") #Добавляем пустую строку\n",
    "        \n",
    "    assert len(contexts) == len(batch['context']), f\"context not quals size | {len(contexts)} {len(batch['context'])}\"\n",
    "    assert len(questions) == len(batch['question']), f\"question not quals size {len(questions)} {len(batch['question'])}\"\n",
    "    assert len(answers) == len(batch['answers']), f\"answers not quals size {len(answers)} {len(batch['answers'])}\"\n",
    "\n",
    "    return { \"context\": contexts, \"question\": questions, \"answers\" : answers }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3212ec7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6552fa064d4f4182aaab69c75ca07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_short_ds = train_dataset.map(\n",
    "    preprocess_dataset_split_context, \n",
    "    batched=True, \n",
    "    num_proc=1,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    cache_file_name=\"./cdatasets/train_ds_split_contex256.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6b8f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26d3a66779e4f489243746833dd529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_short_ds = val_dataset.map(\n",
    "    preprocess_dataset_split_context,\n",
    "    batched=True,       \n",
    "    num_proc=1,  \n",
    "    remove_columns=val_dataset.column_names,  # Удаляем исходные колонки\n",
    "    cache_file_name=\"./cdatasets/val_ds_split_contex256.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c9e0e",
   "metadata": {},
   "source": [
    "### Удалим строки с пустным контекстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa5cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130319\n",
      "11873\n"
     ]
    }
   ],
   "source": [
    "assert len(train_dataset) == len(train_short_ds), \"size train_dataset and train_short_ds not equals\"\n",
    "assert len(val_dataset) == len(val_short_ds), \"size val_dataset and val_short_ds not equals\"\n",
    "print(len(train_short_ds))\n",
    "print(len(val_short_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0614afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Подсчитаем сколько пустых контекстов в Train and Val\n",
    "cnt_train = 0\n",
    "for short in train_short_ds:\n",
    "    if short['context'] == \"\":\n",
    "        cnt_train += 1\n",
    "\n",
    "cnt_val = 0\n",
    "for short in val_short_ds:\n",
    "    if short['context'] == \"\":\n",
    "        cnt_val += 1\n",
    "\n",
    "print(cnt_train) #214\n",
    "print(cnt_val) #14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc38eb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245a1cfcf76b43cb9be8dac8d9a4f5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d869a756fd74b2f8faa7a5a1f5682bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds_short_filter = train_short_ds.filter(lambda row: False if row['context'] == \"\" else True)\n",
    "val_ds_short_filter = val_short_ds.filter(lambda row: False if row['context'] == \"\" else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cba6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130105\n",
      "11859\n",
      "214\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds_short_filter))\n",
    "print(len(val_ds_short_filter))\n",
    "\n",
    "print(len(train_short_ds) - len(train_ds_short_filter))\n",
    "print(len(val_short_ds) - len(val_ds_short_filter))\n",
    "\n",
    "assert len(train_short_ds) - cnt_train == len(train_ds_short_filter), \"Not equal Train\"\n",
    "assert len(val_short_ds) - cnt_val == len(val_ds_short_filter), \"Not equal Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af5189d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\",\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': 'in the late 1990s'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_short_filter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1dd814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.',\n",
       " 'question': 'In what country is Normandy located?',\n",
       " 'answers': 'France'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds_short_filter[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e584d",
   "metadata": {},
   "source": [
    "### Сохраним предобработанные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d344f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b145d27a80054a2ea65c985392df1eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/130105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e957b2e2424928a39f5e83341c0115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds_short_filter.save_to_disk(f\"{OUT_DATA_CACHE}/train_ds_short\")\n",
    "val_ds_short_filter.save_to_disk(f\"{OUT_DATA_CACHE}/val_ds_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51f72e-4184-4d28-b70c-33ee4b5a5aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
