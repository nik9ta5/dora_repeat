{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eddd6a7",
   "metadata": {},
   "source": [
    "## В данном блокноте выполняется предобработка датасета\n",
    "\n",
    "Сокращение длинны контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk #Загрузка датасета \n",
    "\n",
    "# ----------- for ds preprocess -----------\n",
    "import nltk\n",
    "nltk.data.path.clear()\n",
    "nltk.data.path.append('C:/Users/nikita/Desktop/mlRes/nltk_data')\n",
    "nltk.download('punkt', download_dir=\"C:/Users/nikita/Desktop/mlRes/nltk_data\")\n",
    "nltk.download('punkt_tab', download_dir=\"C:/Users/nikita/Desktop/mlRes/nltk_data\")\n",
    "\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- | Параметры | -----\n",
    "\n",
    "OUT_DATA_CACHE = \"./prep_datasets\" #Директория, с предобработанными датасетами\n",
    "CACHE_DIR = \"../myDoRA_repeat/cache_dir\" #Директория с кешем модели\n",
    "EVAL_MODEL_DIR_OUT = \"./eval_models_out\" #Для вывода логов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773dffe",
   "metadata": {},
   "source": [
    "### Dataset - SQuAD 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем датасет\n",
    "datasetSQUAD2 = load_dataset(\"rajpurkar/squad_v2\", cache_dir=\"./cdatasets\")\n",
    "\n",
    "val_dataset = datasetSQUAD2['validation'] \n",
    "train_dataset = datasetSQUAD2['train']\n",
    "\n",
    "#for test pipeline\n",
    "small_train_ds = train_dataset.select(range(32))\n",
    "small_val_ds = val_dataset.select(range(32))\n",
    "\n",
    "print(val_dataset.info)\n",
    "print(train_dataset.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ecc49",
   "metadata": {},
   "source": [
    "### Предобработка датасета \n",
    "\n",
    "Тестировал прямой проход модели на батче из 1 последовательности (1024 токена), работала, но видно с обратным распространением и шагом оптимизатора, памяти не хватает, следовательно,нужно сократить длинну последовательности до 256-512 токенов.\n",
    "\n",
    "Возьму примерно 256 токенов (контекст обрезается по предложениям, берется текущее предыдушее и следущее).\n",
    "\n",
    "Инференс на 1024 токена работает хорошо (причем я указываю что использую внимани \"eager\" для FT и \"sdpa\" для инференса, но не уверен, что оно корректно используется, не до конца протестировал)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cae191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_split_context(batch, neighbors = 1):\n",
    "    \"\"\" Функция для обработки датасета SQuAD 2.0\n",
    "    Для извлечения context, question, answer. И обрезки context до длинны 4-5 предложений.\n",
    "    \n",
    "    Статический срез, берет предыдушее предложение, текущее и следущее (index - neighbors : index + neighbors + 1)\n",
    "\n",
    "    Args:\n",
    "        batch : dict - словарь списков с соответствующими ключами dataset'a\n",
    "    \"\"\"\n",
    "    #Есть строки, в которых нет ответов (в контексте не предоставлены ответы)\n",
    "    #Необходимо, чтобы все ровно соответстовали тому, что было\n",
    "    contexts = [] #Список контекстов\n",
    "    questions = [] #Список воспросов\n",
    "    answers = [] #Список ответов\n",
    "\n",
    "    for cntxt, quest, answer in zip(batch['context'], batch['question'], batch['answers']):\n",
    "        cntx_split_sentenses = nltk.sent_tokenize(cntxt, language='english') #Разбиваем контест по предложениям\n",
    "\n",
    "        if len(answer['text']) > 0: #Ответ есть\n",
    "            found = False\n",
    "            for sent_index in range(len(cntx_split_sentenses)):\n",
    "                if (answer['text'][0] in cntx_split_sentenses[sent_index]): #Если ответ содержится в предложении\n",
    "                    start = max(0, sent_index - neighbors)\n",
    "                    end = min(len(cntx_split_sentenses), sent_index + neighbors + 1)\n",
    "                    res_text_context = \" \".join(cntx_split_sentenses[start : end])\n",
    "\n",
    "                    contexts.append(res_text_context)\n",
    "                    questions.append(quest)\n",
    "                    answers.append(answer['text'][0]) #Добавляем именно текст\n",
    "                    found = True\n",
    "                    break #ОБЯЗАТЕЛЬНО НУЖНО ВЫЙТИ из цикла\n",
    "            if not found:\n",
    "                contexts.append(\"\") #Если не нашли, будет пустой контекст (не будем использовать для обучения и оценки)\n",
    "                questions.append(quest)\n",
    "                answers.append(answer['text'][0]) #Добавляем именно текст\n",
    "        else:\n",
    "            #Нужно добавить контекст как строку, а не как список ;D\n",
    "            contexts.append(\" \".join(cntx_split_sentenses[:3])) #Берем 3 первых предложения\n",
    "            questions.append(quest)\n",
    "            answers.append(\"\") #Добавляем пустую строку\n",
    "        \n",
    "    assert len(contexts) == len(batch['context']), f\"context not quals size | {len(contexts)} {len(batch['context'])}\"\n",
    "    assert len(questions) == len(batch['question']), f\"question not quals size {len(questions)} {len(batch['question'])}\"\n",
    "    assert len(answers) == len(batch['answers']), f\"answers not quals size {len(answers)} {len(batch['answers'])}\"\n",
    "\n",
    "    return { \"context\": contexts, \"question\": questions, \"answers\" : answers }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_short_ds = train_dataset.map(\n",
    "    preprocess_dataset_split_context, \n",
    "    batched=True, \n",
    "    num_proc=1,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    cache_file_name=\"./cdatasets/train_ds_split_contex256.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_short_ds = val_dataset.map(\n",
    "    preprocess_dataset_split_context,\n",
    "    batched=True,       \n",
    "    num_proc=1,  \n",
    "    remove_columns=val_dataset.column_names,  # Удаляем исходные колонки\n",
    "    cache_file_name=\"./cdatasets/val_ds_split_contex256.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c9e0e",
   "metadata": {},
   "source": [
    "### Удалим строки с пустным контекстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_dataset) == len(train_short_ds), \"size train_dataset and train_short_ds not equals\"\n",
    "assert len(val_dataset) == len(val_short_ds), \"size val_dataset and val_short_ds not equals\"\n",
    "print(len(train_short_ds))\n",
    "print(len(val_short_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчитаем сколько пустых контекстов в Train and Val\n",
    "cnt_train = 0\n",
    "for short in train_short_ds:\n",
    "    if short['context'] == \"\":\n",
    "        cnt_train += 1\n",
    "\n",
    "cnt_val = 0\n",
    "for short in val_short_ds:\n",
    "    if short['context'] == \"\":\n",
    "        cnt_val += 1\n",
    "\n",
    "print(cnt_train) #214\n",
    "print(cnt_val) #14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_short_filter = train_short_ds.filter(lambda row: False if row['context'] == \"\" else True)\n",
    "val_ds_short_filter = val_short_ds.filter(lambda row: False if row['context'] == \"\" else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ds_short_filter))\n",
    "print(len(val_ds_short_filter))\n",
    "\n",
    "print(len(train_short_ds) - len(train_ds_short_filter))\n",
    "print(len(val_short_ds) - len(val_ds_short_filter))\n",
    "\n",
    "assert len(train_short_ds) - cnt_train == len(train_ds_short_filter), \"Not equal Train\"\n",
    "assert len(val_short_ds) - cnt_val == len(val_ds_short_filter), \"Not equal Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_short_filter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds_short_filter[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e584d",
   "metadata": {},
   "source": [
    "### Сохраним предобработанные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d344f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_short_filter.save_to_disk(f\"{OUT_DATA_CACHE}/train_ds_short\")\n",
    "val_ds_short_filter.save_to_disk(f\"{OUT_DATA_CACHE}/val_ds_short\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
