{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0759ce25",
   "metadata": {},
   "source": [
    "## Воспроизведение результатов работы - DoRA: Weight-Decomposed Low-Rank Adaptation \n",
    "- [Archive](https://arxiv.org/html/2402.09353v6) \n",
    "- [GitHub](https://github.com/NVlabs/DoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b570562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ce27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 0\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"google/gemma-3-1b-it\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "CACHE_DIR = \"./cache_dir\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.device(DEVICE)\n",
    "\n",
    "print(\n",
    "    DEVICE,\n",
    "    torch.cuda.current_device()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fd986",
   "metadata": {},
   "source": [
    "#### Загрузка токенайзера и модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd76f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, cache_dir=CACHE_DIR)\n",
    "\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b8de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9e6d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "243e7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025759744\n",
      "3839.263671875 MB\n",
      "3930.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(str(torch.cuda.memory_allocated() / 1024 ** 2) + \" MB\")\n",
    "print(str(torch.cuda.memory_reserved() / 1024 ** 2) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12a3a0",
   "metadata": {},
   "source": [
    "### DoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad0ce8",
   "metadata": {},
   "source": [
    ".peft скопированна с репозитория - https://github.com/NVlabs/DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b82d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"peft/src/\"))\n",
    "from peft import (  # noqa: E402\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    DoraConfig,\n",
    "    BottleneckConfig,\n",
    "    PrefixTuningConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Определяем конфигурацию для DoRa\n",
    "dora_config = DoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,  # Масштабирующий коэффициент\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Модули для тонкой настройки (например, attention слои)\n",
    "    lora_dropout=0.05,  # Дропаут\n",
    "    bias=\"none\",  # Без смещения\n",
    "    task_type=\"CAUSAL_LM\",  # Тип задачи\n",
    "    dora_simple=True,  # Упрощённый режим DoRA\n",
    "    Wdecompose_target_modules=[\"q_proj\", \"v_proj\"]  # Модули для декомпозиции весов\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fe6ff-28f7-4282-83af-3ccdc94d33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "data_path = \"./dataset/hellaswag\"\n",
    "cutoff_len = 32 # Максимальная длина последовательности\n",
    "val_set_size = 2000  # Размер валидационной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Генерация промпта\n",
    "def generate_prompt(data_point):\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction: {data_point[\"instruction\"]}\n",
    "### Input: {data_point[\"input\"]}\n",
    "### Response: {data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction: {data_point[\"instruction\"]}\n",
    "### Response: {data_point[\"output\"]}\"\"\"\n",
    "\n",
    "# Токенизация с генерацией промпта\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d47f66",
   "metadata": {},
   "source": [
    "#### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0927541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 39905\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 10042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295f6d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'answer'],\n",
      "        num_rows: 37905\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'answer'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980b095652b247f688b7f05a4ac58f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d92a3ac5fe4b75ba9cf4a5ba3f0f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if val_set_size > 0:\n",
    "    #Разделили на часть обучени и валидации\n",
    "    train_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\n",
    "    print(train_val)\n",
    "    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bf1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training(model, use_gradient_checkpointing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea063e30-53c3-430c-aaf7-b50f066566bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    prompts = [generate_prompt(instruction) for instruction in examples[\"instruction\"]]\n",
    "    inputs = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    # Метки — это правильные ответы (например, ending1, ending2 и т.д.)\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"labels\": examples[\"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a881104e-c75b-496e-b33e-7a1350fd2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    pred_answers = re.findall(r'ending1|ending2|ending3|ending4', sentence)\n",
    "    return pred_answers[0] if pred_answers else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8c88c",
   "metadata": {},
   "source": [
    "#### Trainer для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "micro_batch_size = 16\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "eval_step = 2000000 #Данное значение взято специально, с расчетом не производить вычисление метрики приобучении\n",
    "save_step = 2000000\n",
    "output_dir = \"./dora-ngemma\" #Директория для сохранения результата - конфига и самого адаптера\n",
    "\n",
    "# Инициализация Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.0,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_step if val_set_size > 0 else None,\n",
    "        save_steps=save_step,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False\n",
    "    ),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Отключение кэша для обучения\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29964235",
   "metadata": {},
   "source": [
    "#### Обучение завершилось, но при попытке оценки модели, во время обучения, возникали ошибки. Во время обучения было принято решение не оценивать качество модели (не вычислять метрику), а оценить после. Оценка полученного результата представлена ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd497e21-589e-407c-ab03-6e1315ba3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7111' max='7110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7110/7110 07:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30/250 00:02 < 00:21, 10.28 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.model.embed_tokens.weight', 'base_model.model.lm_head.0.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Обучение\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/transformers/trainer.py:2627\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2626\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2627\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2638\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/transformers/trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/transformers/trainer.py:3200\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3198\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3199\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3203\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/transformers/trainer.py:3902\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3899\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3902\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3904\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/transformers/trainer.py:4000\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3998\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_safetensors:\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m     \u001b[43msafetensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4004\u001b[39m     torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/safetensors/torch.py:488\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    485\u001b[39m         failing.append(names)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[33m        A potential way to correctly save your model is to use `save_model`.\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[33m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[31mRuntimeError\u001b[39m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.model.embed_tokens.weight', 'base_model.model.lm_head.0.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27ec669a-6016-4fe0-bbf7-28bb78970eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976855ab-2373-43fe-8ab3-1e4d04190f19",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "Оценка производилась на тестовой части бенчмарка HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c0f3e3a-7423-4bcd-8453-ac024659792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./dora-ngemma\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6200cf08",
   "metadata": {},
   "source": [
    "#### Загрузка адаптера для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17015ae5-07e1-4958-a0ab-4a8369f0fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.0.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.1.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.1.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.2.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.2.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.3.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.3.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.4.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.4.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.5.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.5.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.6.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.6.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.7.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.7.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.8.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.8.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.9.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.9.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.10.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.10.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.11.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.11.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.12.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.12.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.13.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.13.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.14.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.14.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.15.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.15.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.16.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.16.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.17.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.17.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.18.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.18.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.19.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.19.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.20.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.20.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.21.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.21.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.22.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.22.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.23.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.23.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.24.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.24.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.25.self_attn.q_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight is trainable\n",
      "base_model.model.model.layers.25.self_attn.v_proj.weight_m_wdecomp.weight is trainable\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight is trainable\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight is trainable\n"
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, output_dir) #Загрузка адаптера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bf0c4ce-475b-4513-b651-08ed3cfb3069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): DoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): DoraModel(\n",
       "        (model): Gemma3ForCausalLM(\n",
       "          (model): Gemma3TextModel(\n",
       "            (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "            (layers): ModuleList(\n",
       "              (0-25): 26 x Gemma3DecoderLayer(\n",
       "                (self_attn): Gemma3Attention(\n",
       "                  (q_proj): Linear(\n",
       "                    in_features=1152, out_features=1024, bias=False\n",
       "                    (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                    (weight_m_wdecomp): Linear(in_features=1, out_features=1024, bias=False)\n",
       "                    (lora_A): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                    (lora_B): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "                  (v_proj): Linear(\n",
       "                    in_features=1152, out_features=256, bias=False\n",
       "                    (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                    (weight_m_wdecomp): Linear(in_features=1, out_features=256, bias=False)\n",
       "                    (lora_A): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                    (lora_B): Linear(in_features=16, out_features=256, bias=False)\n",
       "                  )\n",
       "                  (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "                  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                )\n",
       "                (mlp): Gemma3MLP(\n",
       "                  (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "                  (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "                  (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "                  (act_fn): PytorchGELUTanh()\n",
       "                )\n",
       "                (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "              )\n",
       "            )\n",
       "            (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (rotary_emb): Gemma3RotaryEmbedding()\n",
       "            (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "          )\n",
       "          (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75f60a",
   "metadata": {},
   "source": [
    "#### Загрузка тестовой части бенчмарка и преобразование (формирование промпта и токенизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13570b3-94ff-4888-8e68-17b83ded1687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59517429821142609b619e2e934dafeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = data[\"test\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b5ff4",
   "metadata": {},
   "source": [
    "#### Функция для вычисления accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6c3d5-ed6e-4bf6-a5af-14032ab2e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataset, tokenizer, device):\n",
    "    dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            instructions = batch['instruction']\n",
    "            labels = batch['answer']\n",
    "\n",
    "            tokenized_inputs = tokenizer(instructions, return_tensors='pt', padding=True, truncation=True)\n",
    "            tokenized_inputs = {key: value.to(device) for key, value in tokenized_inputs.items()}\n",
    "\n",
    "            outputs = model(input_ids=tokenized_inputs['input_ids'],\n",
    "                            attention_mask=tokenized_inputs['attention_mask'])\n",
    "\n",
    "            predicted_token_ids = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            predictions = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
    "            predictions = [extract_answer(pred.split(\"### Response:\")[-1].strip()) for pred in predictions]\n",
    "\n",
    "            correct += sum(pred == label for pred, label in zip(predictions, labels))\n",
    "            total += len(labels)\n",
    "            cnt += 1\n",
    "\n",
    "            if cnt % 200 == 0:\n",
    "                print(f\"num batch: {cnt*2}, accuracy: {correct / total}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdc2ac58-0028-4609-9e95-102858c96019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batch: 400, accuracy: 0.235\n",
      "num batch: 800, accuracy: 0.24875\n",
      "num batch: 1200, accuracy: 0.24916666666666668\n",
      "num batch: 1600, accuracy: 0.251875\n",
      "num batch: 2000, accuracy: 0.2565\n",
      "num batch: 2400, accuracy: 0.2525\n",
      "num batch: 2800, accuracy: 0.24857142857142858\n",
      "num batch: 3200, accuracy: 0.253125\n",
      "num batch: 3600, accuracy: 0.2577777777777778\n",
      "num batch: 4000, accuracy: 0.2565\n",
      "num batch: 4400, accuracy: 0.25272727272727274\n",
      "num batch: 4800, accuracy: 0.24916666666666668\n",
      "num batch: 5200, accuracy: 0.24903846153846154\n",
      "num batch: 5600, accuracy: 0.24660714285714286\n",
      "num batch: 6000, accuracy: 0.24566666666666667\n",
      "num batch: 6400, accuracy: 0.24671875\n",
      "num batch: 6800, accuracy: 0.24661764705882352\n",
      "num batch: 7200, accuracy: 0.24847222222222223\n",
      "num batch: 7600, accuracy: 0.2488157894736842\n",
      "num batch: 8000, accuracy: 0.246375\n",
      "num batch: 8400, accuracy: 0.24654761904761904\n",
      "num batch: 8800, accuracy: 0.24806818181818183\n",
      "num batch: 9200, accuracy: 0.2501086956521739\n",
      "num batch: 9600, accuracy: 0.2525\n",
      "num batch: 10000, accuracy: 0.2511\n",
      "Accuracy: 0.25114519020115517\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = compute_accuracy(model, test_data, tokenizer, DEVICE)\n",
    "print(f\"Accuracy: {accuracy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b936c9",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "\n",
    "- Можно заметить очень низкое значение метрики модели (0.25)\n",
    "- Размер рассмотренной модели существенно меньше, моделей в работе (Llama 7B, 8B, 13B, против Gemma 1B). Выбор модели основывался чисто на требованиях к ресурсам, на какой модели можно было быстро проверить работоспособность\n",
    "- Для настройки были выбраны следующие параметры: r = 16, lr = 2e-4, количество эпох = 3\n",
    "- Размер обучающих данных очень маленький (около 50 мб). В исследовании подчеркивается, что создавался общий датасет (обучение производилось на всех рассмотренных бенчмарках сразу)\n",
    "- Во время обучения не производилась оценка модели, что позволило бы остановить и скорректировать процесс обучения (нужно доработать)\n",
    "- В результате работы был получен baseline, который можно применить в дальнейшем для тонкой настройки и оценки модели. Не получилось получить результатов, по метрикам, сопостовимых с результатами работы (https://github.com/NVlabs/DoRA), но дальнейшие эксперименты, возможно, приблизят к ним."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
