2025-05-23 09:03:20,536 - INFO - 
Title: This step train model with use LoRA
Process: Train model

--- Parameters Training ---
MODEL: google/gemma-3-1b-it
DEVICE: cuda:0
max_len_tokens: 256
new_max_tokens: 16
TEMPERATURE: 0.7

BATCH_SIZE: 8
BATCH_SIZE_EVAL: 8

PADDING_SIDE_TOKENIZER: left

OUT_DATA_CACHE: ../ft_v1/prep_datasets
CACHE_DIR: ../myDoRA_repeat/cache_dir
EVAL_MODEL_DIR_OUT: ./eval_models_out

Train data: ../ft_v1/prep_datasets/train_ds_short
Val data: ../ft_v1/prep_datasets/val_ds_short

--- Parameters for LORA ---
TRAINED PARAMETERS: 2981888

EPOCH: 1
LEARNING_RATE: 1e-05
GRADIENT_ACCUMULATION_STEPS: 1
TRAIN_LOGG_STEP: 1000
SAVE_MODEL_DIR_LORA: ./lora_ft_squad2
SAVE_MODEL_DIR_DORA: ./dora_ft_squad2

--- Parameters for optimizer ---
BETAS: (0.9, 0.99)
EPS: 1e-08

--- Model architecture ---
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): Gemma3ForCausalLM(
      (model): Gemma3TextModel(
        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)
        (layers): ModuleList(
          (0-25): 26 x Gemma3DecoderLayer(
            (self_attn): Gemma3Attention(
              (q_proj): Linear(
                in_features=1152, out_features=1024, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_A): Linear(in_features=1152, out_features=32, bias=False)
                (lora_B): Linear(in_features=32, out_features=1024, bias=False)
              )
              (k_proj): Linear(in_features=1152, out_features=256, bias=False)
              (v_proj): Linear(
                in_features=1152, out_features=256, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_A): Linear(in_features=1152, out_features=32, bias=False)
                (lora_B): Linear(in_features=32, out_features=256, bias=False)
              )
              (o_proj): Linear(in_features=1024, out_features=1152, bias=False)
              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)
              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)
            )
            (mlp): Gemma3MLP(
              (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)
              (up_proj): Linear(in_features=1152, out_features=6912, bias=False)
              (down_proj): Linear(in_features=6912, out_features=1152, bias=False)
              (act_fn): PytorchGELUTanh()
            )
            (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)
            (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)
            (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)
            (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)
          )
        )
        (norm): Gemma3RMSNorm((1152,), eps=1e-06)
        (rotary_emb): Gemma3RotaryEmbedding()
        (rotary_emb_local): Gemma3RotaryEmbedding()
      )
      (lm_head): Linear(in_features=1152, out_features=262144, bias=False)
    )
  )
)

--- Model config ---
Gemma3TextConfig {
  "_attn_implementation_autoset": true,
  "architectures": [
    "Gemma3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": null,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "final_logit_softcapping": null,
  "head_dim": 256,
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 1152,
  "initializer_range": 0.02,
  "intermediate_size": 6912,
  "max_position_embeddings": 32768,
  "model_type": "gemma3_text",
  "num_attention_heads": 4,
  "num_hidden_layers": 26,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_local_base_freq": 10000,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": 512,
  "sliding_window_pattern": 6,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": false,
  "vocab_size": 262144
}


